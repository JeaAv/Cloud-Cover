{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnrDA9C224r7",
        "outputId": "c0719847-c411-473d-abf7-b167e42442c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NcqXLRZKVMmg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/SIA/HRC_WHU\""
      ],
      "metadata": {
        "id": "u3ItOai-ebe5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TIFF image\n",
        "img_path = 'image.tif'\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# Check if image is loaded correctly\n",
        "if img is None:\n",
        "    print(f\"Error: Unable to load image '{img_path}'\")\n",
        "else:\n",
        "    # Preprocess image\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply OCR\n",
        "    text = pytesseract.image_to_string(gray)\n",
        "\n",
        "    # Save text to file\n",
        "    with open('output.txt', 'w') as f:\n",
        "        f.write(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg2REVaq3QwX",
        "outputId": "4f9f8453-6955-472c-afb3-8fa39f1983cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Unable to load image 'image.tif'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder path containing TIFF files\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/SIA/HRC_WHU'\n",
        "\n",
        "# Loop through TIFF files and convert to JPG\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".tif\"):\n",
        "        img = cv2.imread(os.path.join(folder_path, filename))\n",
        "        cv2.imwrite(os.path.join(folder_path, filename.replace(\".tif\", \".jpg\")), img)\n"
      ],
      "metadata": {
        "id": "de1vSyTncEb1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "image_paths = glob.glob(folder_path + '/*.jpg')\n",
        "df = pd.DataFrame({'image_path': image_paths})"
      ],
      "metadata": {
        "id": "Oy_QEzcKdUNC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert images to text using OCR\n",
        "def image_to_text(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    text = pytesseract.image_to_string(gray)\n",
        "    return text"
      ],
      "metadata": {
        "id": "cisRicuFgszD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply OCR to images\n",
        "df['text'] = df['image_path'].apply(image_to_text)"
      ],
      "metadata": {
        "id": "daKQaL62gzAI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Assuming labels are based on the filename 'label_image.jpg', need to extract them.\n",
        "def extract_label(image_path):\n",
        "    filename = os.path.basename(image_path)  # Get filename from path\n",
        "    label = filename.split('_')[0]  # Split filename and take the first part\n",
        "    return label\n",
        "\n",
        "# Apply the function to create the 'label' column\n",
        "df['label'] = df['image_path'].apply(extract_label)\n",
        "\n",
        "# NLP preprocessing\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "y = df['label']"
      ],
      "metadata": {
        "id": "jOLZFZQrg1xw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RsnkOM2jg4vQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train NLP model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyGJGLrxg-9k",
        "outputId": "418450a9-9172-4f1d-e8aa-2665979a9617"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(df['text'], y, test_size=0.2, random_state=42) # Split the original text data\n",
        "\n",
        "# Train NLP model\n",
        "# Now encode the text data\n",
        "train_encodings = tokenizer.batch_encode_plus(X_train_text.tolist(), add_special_tokens=True, max_length=512, return_attention_mask=True, return_tensors='pt', padding='max_length', truncation=True) # Use tolist() to convert to a list\n",
        "test_encodings = tokenizer.batch_encode_plus(X_test_text.tolist(), add_special_tokens=True, max_length=512, return_attention_mask=True, return_tensors='pt', padding='max_length', truncation=True) # Use tolist() to convert to a list"
      ],
      "metadata": {
        "id": "jugs1PGghFZS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "U6y8qoXThJEw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Create a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to your training labels and transform them\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)  # Transform test labels using the same encoder\n",
        "\n",
        "# Train model or use y_train_encoded and y_test_encoded in the training loop:\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(train_encodings['input_ids'].to(device), attention_mask=train_encodings['attention_mask'].to(device), labels=torch.tensor(y_train_encoded).to(device)) # Use encoded labels\n",
        "    loss = criterion(outputs.logits, torch.tensor(y_train_encoded).to(device)) # Use encoded labels\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "Ndy1-VuxhNj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_encodings['input_ids'].to(device), attention_mask=test_encodings['attention_mask'].to(device))\n",
        "    logits = outputs.logits\n",
        "    predicted = torch.argmax(logits, dim=1)\n",
        "    accuracy = accuracy_score(y_test, predicted.cpu().numpy())\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(classification_report(y_test, predicted.cpu().numpy()))"
      ],
      "metadata": {
        "id": "r3qIvjZWhSei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "plt.bar(df['label'].value_counts().index, df['label'].value_counts())\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Label Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "siLh1s89hXWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}